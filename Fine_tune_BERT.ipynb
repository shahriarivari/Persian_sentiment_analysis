{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbDfWcLrlaPzP9suUcnfIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahriarivari/Persian_sentiment_analysis/blob/main/Fine_tune_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pip installs"
      ],
      "metadata": {
        "id": "0RE70MtNwwwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFrQ_TNqwGQQ"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers\n",
        "!pip install datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U transformers\n",
        "!pip install hazm\n",
        "!pip install stopwords_guilannlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##import libraries"
      ],
      "metadata": {
        "id": "R68TDDOAw4k3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader , Dataset\n",
        "\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import codecs\n",
        "\n",
        "# Preprocessing\n",
        "from stopwords_guilannlp import stopwords_output\n",
        "from hazm import *\n",
        "\n",
        "# Visualization\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import plot_model\n",
        "# Measuring metrics\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "-XFlnDTsw6do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "TgkUCTLN02Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import and analyze SA Dataset"
      ],
      "metadata": {
        "id": "iKDgMWmIzQuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('test.csv', index_col=None, header=None, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "iGT4IvF9zR1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a binarray dataset since the dataset has mutiple labels\n",
        "\n",
        "binary_y_test = []\n",
        "binary_x_test = []\n",
        "for i, y in enumerate(y_test):\n",
        "  if int(y) != 0:\n",
        "    if int(y) > 0:\n",
        "      binary_y_test.append(1)\n",
        "      binary_x_test.append(x_test[i])\n",
        "    else:\n",
        "      binary_y_test.append(0)\n",
        "      binary_x_test.append(x_test[i])\n",
        "\n",
        "# convert them into np arrays\n",
        "x_test = np.asarray(binary_x_test)\n",
        "y_test = np.asarray(binary_y_test)"
      ],
      "metadata": {
        "id": "7McLEhXwzTkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import other datasets for sentiment analysis"
      ],
      "metadata": {
        "id": "KuIOdlDbzUnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing othere datasets\n",
        "original = pd.read_csv('original.csv', index_col=None, header=None, encoding=\"utf-8\")\n",
        "balanced = pd.read_csv('balanced.csv', index_col=None, header=None, encoding=\"utf-8\")\n",
        "translation = pd.read_csv('translation.csv', index_col=None, header=None, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "O0RntiFwzZD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we select a dataset to work with\n",
        "selected_dataset = original\n",
        "# shuffle the dataframe\n",
        "selected_dataset = selected_dataset.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "cCwey1y6zaOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating train data\n",
        "x_train = selected_dataset[0]\n",
        "y_train = selected_dataset[1]\n",
        "print('Number of training sentence: ', x_train.shape)\n",
        "print('Number of training label: ', y_train.shape)"
      ],
      "metadata": {
        "id": "jPz2g_W8zbQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# again make our dataset binarray\n",
        "\n",
        "binary_y_train = []\n",
        "binary_x_train = []\n",
        "for i, y in enumerate(y_train):\n",
        "  if int(y) != 0:\n",
        "    if int(y) > 0:\n",
        "      binary_y_train.append(1)\n",
        "      binary_x_train.append(x_train[i])\n",
        "    else:\n",
        "      binary_y_train.append(0)\n",
        "      binary_x_train.append(x_train[i])\n",
        "\n",
        "# Convert dataframes to numpy arrays\n",
        "x_train = np.asarray(binary_x_train)\n",
        "y_train = np.asarray(binary_y_train)"
      ],
      "metadata": {
        "id": "6Tdzb05fzcWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Plot some information about our downstream task dataset"
      ],
      "metadata": {
        "id": "UfsMTh0uzenj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# See the data number of sentence in each category\n",
        "cnt = Counter(y_train)\n",
        "cnt = dict(cnt)\n",
        "print(cnt)\n",
        "\n",
        "labels = list(cnt.keys())\n",
        "sizes = list(cnt.values())\n",
        "colors = ['#3fba36', '#66b3ff','#ffcc99','#ff9999', '#d44444']\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', startangle=90)\n",
        "#draw circle\n",
        "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(centre_circle)\n",
        "# Equal aspect ratio ensures that pie is drawn as a circle\n",
        "ax1.axis('equal')\n",
        "plt.tight_layout()\n",
        "# Decomment following line if you want to save the figure\n",
        "# plt.savefig('distribution.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "00wM6XIIze70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Preprocessing"
      ],
      "metadata": {
        "id": "pj_A9TFOzhGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "puncs = ['ØŒ', '.', ',', ':', ';', '\"']\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    doc = normalizer.normalize(doc) # Normalize document using Hazm Normalizer\n",
        "    tokenized = word_tokenize(doc)  # Tokenize text\n",
        "    tokens = []\n",
        "    for t in tokenized:\n",
        "      temp = t\n",
        "      for p in puncs:\n",
        "        temp = temp.replace(p, '')\n",
        "      tokens.append(temp)\n",
        "    # tokens = [w for w in tokens if not w in stop_set]    # Remove stop words\n",
        "    tokens = [w for w in tokens if not len(w) <= 1]\n",
        "    tokens = [w for w in tokens if not w.isdigit()]\n",
        "    tokens = [lemmatizer.lemmatize(w) for w in tokens] # Lemmatize sentence words using Hazm Lemmatizer\n",
        "    tokens = ' '.join(tokens)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "y0jft9Myzi5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing step to training data\n",
        "train_docs = np.empty_like(x_train)\n",
        "for index, document in enumerate(x_train):\n",
        "  train_docs[index] = clean_doc(document)\n",
        "\n",
        "# Applying preprocessing step to test data\n",
        "test_docs = np.empty_like(x_test)\n",
        "for index, document in enumerate(x_test):\n",
        "  test_docs[index] = clean_doc(document)"
      ],
      "metadata": {
        "id": "w0m-kkeOzkF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the pretrained model and tokenizer"
      ],
      "metadata": {
        "id": "RDJnHBAC0YRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount to google drive\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "zip_file_path = '/content/drive/MyDrive/bert_pretrained_model.zip'\n",
        "# Create a directory to extract the contents\n",
        "extract_path = '/content/bert_pretrained_model/'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "lZJAGtRE0a6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved BERT model\n",
        "bert_model = BertModel.from_pretrained(os.path.join(\"bert_pretrained_model\", \"final_model\"))"
      ],
      "metadata": {
        "id": "C_X1XW9q0o1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_output_dir = \"bert_tokenizer\"\n",
        "os.mkdir(tokenizer_output_dir)"
      ],
      "metadata": {
        "id": "gJYwvblH0tLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_output_dir, max_len = 512)"
      ],
      "metadata": {
        "id": "AUfp3CEn0ur0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##classifiers"
      ],
      "metadata": {
        "id": "Q-tWF6Eh07dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###MLP"
      ],
      "metadata": {
        "id": "eu1GfjAf1DJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your MLP classifier\n",
        "class MLPClassifier(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        # nn.Linear(hidden_size, 32),\n",
        "        # nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_size),\n",
        "        #nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp(x)"
      ],
      "metadata": {
        "id": "cO7fhJHT09me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##tokenizing data"
      ],
      "metadata": {
        "id": "OUn4PADZ2qpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize train_docs and test_docs using the trained tokenizer\n",
        "train_tokenized = tokenizer(train_docs.tolist(),max_length=512, truncation=True, padding=True,\n",
        "                            return_tensors='pt')\n",
        "\n",
        "test_tokenized = tokenizer(test_docs.tolist(),max_length=512, truncation=True, padding=True,\n",
        "                           return_tensors='pt')"
      ],
      "metadata": {
        "id": "AXKWfff52u54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###create a dataset object"
      ],
      "metadata": {
        "id": "etXn5PYH2yws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.inputs[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.inputs[\"attention_mask\"][idx],\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        }"
      ],
      "metadata": {
        "id": "OoJ39kXu29H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create datalaoders"
      ],
      "metadata": {
        "id": "L5DBKN-f3ByW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "# Create DataLoader\n",
        "train_dataset = CustomDataset(train_tokenized, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Create DataLoader\n",
        "test_dataset = CustomDataset(test_tokenized, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "w7N7-SId3EYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize MLP Classifier and Define Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "_VOnOuAh31uI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize MLP classifier\n",
        "input_size = bert_model.config.hidden_size  # Get the size of the pooled representation\n",
        "hidden_units = 256  # Adjust as needed\n",
        "output_size = 1  # Binary classification, 1 output unit\n",
        "learning_rate = 0.1\n",
        "torch.manual_seed(42)\n",
        "classifier = MLPClassifier(input_size=input_size, hidden_size=hidden_units, output_size=output_size)\n",
        "\n",
        "# remove the pooler layer in bert\n",
        "bert_model.pooler = None\n",
        "\n",
        "# Move your model and data loaders to the selected device\n",
        "bert_model.to(device)\n",
        "classifier.to(device)\n",
        "\n",
        "# Freeze all BERT layers\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# BCE loss\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "# Specify the parameters to be optimized\n",
        "optimizer = torch.optim.AdamW([{'params': classifier.parameters(), 'lr': learning_rate}])"
      ],
      "metadata": {
        "id": "8kcwC7MV32ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training loop\n"
      ],
      "metadata": {
        "id": "mbn8DN-S4CdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accumulation_steps = 8  # Accumulate gradients over 8 batches\n",
        "classifier.eval()\n",
        "\n",
        "for epoch in range(5):\n",
        "\n",
        "    train_loss = 0.0\n",
        "    test_loss = 0.0\n",
        "    print(\"epoch = \", epoch + 1)\n",
        "    print(\"---------------------------------------\")\n",
        "\n",
        "    # Training\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        inputs = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        classifier.train()\n",
        "\n",
        "        bert_outputs = bert_model(input_ids=inputs, attention_mask=attention_mask)\n",
        "        bert_last_hidden_states = bert_outputs.last_hidden_state\n",
        "\n",
        "        classifier_outputs = classifier(bert_last_hidden_states[:, 0, :])\n",
        "        loss = criterion(classifier_outputs, labels.unsqueeze(dim=1))\n",
        "        loss = loss / accumulation_steps  # Normalize the loss if using gradient accumulation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters every accumulation_steps batches\n",
        "        if (batch_idx + 1) % accumulation_steps == 0 or batch_idx == len(train_loader) - 1:\n",
        "            optimizer.zero_grad()\n",
        "            optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss per batch for training\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    print(f\"train loss per batch = {train_loss}\")\n",
        "\n",
        "    # Validation\n",
        "    bert_model.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch_idx, batch in enumerate(test_loader):\n",
        "            inputs = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            bert_outputs = bert_model(input_ids=inputs, attention_mask=attention_mask)\n",
        "            bert_last_hidden_states = bert_outputs.last_hidden_state\n",
        "            classifier_outputs = classifier(bert_last_hidden_states[:, 0, :])\n",
        "            loss = criterion(classifier_outputs, labels.unsqueeze(dim=1))\n",
        "            test_loss += loss.item()\n",
        "\n",
        "        # Calculate average loss per batch for testing\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "        print(f\"test loss per batch = {test_loss}\")\n"
      ],
      "metadata": {
        "id": "LW3-k7Dp4GoK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}